package com.biz.spark.quetions

/**
 * Given following PySpark DataFrame:
 *
 * ð—²ð—ºð—½_ð—¶ð—± | ð—½ð—¿ð—¼ð—·ð—²ð—°ð˜ð˜€
 * 101   | ["P1","P2"]
 * 102   | ["P1"]
 * 103   | ["P2","P3"]
 *
 * Write PySpark code to get each project and the count of distinct employees working on it.
 *
 * ðŸ’¡ ð—˜ð˜…ð—½ð—²ð—°ð˜ð—²ð—± ð—¢ð˜‚ð˜ð—½ð˜‚ð˜:
 *
 * ð—½ð—¿ð—¼ð—·ð—²ð—°ð˜ | ð—²ð—ºð—½ð—¹ð—¼ð˜†ð—²ð—²_ð—°ð—¼ð˜‚ð—»ð˜
 * P1     | 2
 * P2    | 2
 * P3     | 1
 *
 * ð—˜ð˜…ð—½ð—¹ð—®ð—»ð—®ð˜ð—¶ð—¼ð—»:
 * 1. explode() is used to flatten the array column (projects) so each project gets its own row.
 *
 * 2. groupBy("project") groups data by each project.
 *
 * 3. countDistinct("emp_id") ensures that even if an employee appears multiple times, they are counted only once per project.
 *
 * ð—–ð—¼ð—ºð—ºð—¼ð—» ð— ð—¶ð˜€ð˜ð—®ð—¸ð—²:
 * Using count() instead of countDistinct() â€” this can give wrong results if employee IDs are duplicated for any reason.
 * https://www.linkedin.com/posts/shyamala-kashyap_ltimindtree-pyspark-activity-7339147509631471616-j7-i?utm_source=share&utm_medium=member_desktop&rcm=ACoAAANTdC0B5L7FdPiUMotnd03HKA9KrJGzidM
 */
import com.biz.spark.common.SparkConnection.spark
import com.biz.spark.common.SparkConnection.spark.implicits._
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object ProjectWiseEmployeeCount {
  case class Emp(emp_id:String,projects:List[String])
  def main(args: Array[String]): Unit = {
    val data = Seq(Emp("101",List("p1","p2")),Emp("102",List("p1")),Emp("103",List("p2","p3")))
    val df = spark.createDataset(data)
    val projectWiseEmps = df.withColumn("project_id",explode($"projects")).groupBy("project_id").agg(countDistinct("emp_id").as("emp_count"))
    projectWiseEmps.show(100,false)
    val projectWiseEmpsMap = projectWiseEmps.collect().map(row => {
      (row.getAs[String]("project_id"),row.getAs[Long]("emp_count"))
    }).toMap

    projectWiseEmpsMap
  }

  def projectWiseEmployeeCount(data:Seq[Emp]):Map[String,Long] = {
    val data = Seq(Emp("101",List("p1","p2")),Emp("102",List("p1")),Emp("103",List("p2","p3")))
    val df = spark.createDataset(data)
    val projectWiseEmps = df.withColumn("project_id",explode($"projects")).groupBy("project_id").agg(countDistinct("emp_id").as("emp_count"))
    projectWiseEmps.show(100,false)
    projectWiseEmps.collect().map(p => (p.getString(0),p.getAs[Long](1))).toMap
  }
  def projectWiseEmployee(data:Seq[Emp]):DataFrame = {
    val data = Seq(Emp("101",List("p1","p2")),Emp("102",List("p1")),Emp("103",List("p2","p3")))
    val df = spark.createDataset(data)
    val projectWiseEmps = df.withColumn("project_id",explode($"projects")).groupBy("project_id").agg(countDistinct("emp_id").as("emp_count"))
    projectWiseEmps
  }
}
